{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gerador de letras do Kevinho\n",
    "\n",
    "## Esse programa utiliza base de dados do site\n",
    "#Link: https://aneisse.com/post/2019-02-10-music-data-scraping/2019-02-10-music-data-scraping/ que utliza o R para extrais músicas do site www.vagalume.com. \n",
    "\n",
    "As músicas foram salvas no formato txt , depois segui o exemplo do site https://towardsdatascience.com/ai-generates-taylor-swifts-song-lyrics-6fd92a03ef7e que usa LSTM para gerar músicas da Taylor Swifts e utilizei para gerar músicas do Kevinho. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Import the dependencies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys \n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Activation, Flatten, Dropout, Dense, Embedding, TimeDistributed, CuDNNLSTM\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset and convert it to lowercase :\n",
    "textFileName = 'kevinho.txt'\n",
    "raw_text = open(textFileName, encoding = 'UTF-8').read()\n",
    "raw_text = raw_text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping chars to ints :\n",
    "chars = sorted(list(set(raw_text)))\n",
    "int_chars = dict((i, c) for i, c in enumerate(chars))\n",
    "chars_int = dict((i, c) for c, i in enumerate(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Characters :  59498\n",
      "Total Vocab :  64\n"
     ]
    }
   ],
   "source": [
    "n_chars = len(raw_text)\n",
    "n_vocab = len(chars)\n",
    "print('Total Characters : ' , n_chars) # number of all the characters in lyricsText.txt\n",
    "print('Total Vocab : ', n_vocab) # number of unique characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Patterns :  59398\n"
     ]
    }
   ],
   "source": [
    "# process the dataset:\n",
    "seq_len = 100\n",
    "data_X = []\n",
    "data_y = []\n",
    "for i in range(0, n_chars - seq_len, 1):\n",
    "    # Input Sequeance(will be used as samples)\n",
    "    seq_in  = raw_text[i:i+seq_len]\n",
    "    # Output sequence (will be used as target)\n",
    "    seq_out = raw_text[i + seq_len]\n",
    "    # Store samples in data_X\n",
    "    data_X.append([chars_int[char] for char in seq_in])\n",
    "    # Store targets in data_y\n",
    "    data_y.append(chars_int[seq_out])\n",
    "n_patterns = len(data_X)\n",
    "print( 'Total Patterns : ', n_patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape X to be suitable to go into LSTM RNN :\n",
    "X = np.reshape(data_X , (n_patterns, seq_len, 1))\n",
    "# Normalizing input data :\n",
    "X = X/ float(n_vocab)\n",
    "# One hot encode the output targets :\n",
    "y = np_utils.to_categorical(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "LSTM_layer_num = 4 # number of LSTM layers\n",
    "layer_size = [256,256,256,256] # number of nodes in each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(LSTM(layer_size[0], input_shape =(X.shape[1], X.shape[2]), return_sequences = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,LSTM_layer_num) :\n",
    "    model.add(LSTM(layer_size[i], return_sequences=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Dense(y.shape[1]))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 100, 256)          264192    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 100, 256)          525312    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (None, 100, 256)          525312    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 100, 256)          525312    \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 25600)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                1638464   \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 64)                0         \n",
      "=================================================================\n",
      "Total params: 3,478,592\n",
      "Trainable params: 3,478,592\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the checkpoint :\n",
    "checkpoint_name = 'Weights-LSTM-improvement-{epoch:03d}-{loss:.5f}-bigger.hdf5'\n",
    "checkpoint = ModelCheckpoint(checkpoint_name, monitor='loss', verbose = 1, save_best_only = True, mode ='min')\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 47518 samples, validate on 11880 samples\n",
      "Epoch 1/30\n",
      "47518/47518 [==============================] - 117s 2ms/step - loss: 3.0808 - val_loss: 3.0346\n",
      "\n",
      "Epoch 00001: loss improved from inf to 3.08076, saving model to Weights-LSTM-improvement-001-3.08076-bigger.hdf5\n",
      "Epoch 2/30\n",
      "47518/47518 [==============================] - 110s 2ms/step - loss: 3.0543 - val_loss: 3.0405\n",
      "\n",
      "Epoch 00002: loss improved from 3.08076 to 3.05434, saving model to Weights-LSTM-improvement-002-3.05434-bigger.hdf5\n",
      "Epoch 3/30\n",
      "47518/47518 [==============================] - 110s 2ms/step - loss: 3.0519 - val_loss: 3.0451\n",
      "\n",
      "Epoch 00003: loss improved from 3.05434 to 3.05185, saving model to Weights-LSTM-improvement-003-3.05185-bigger.hdf5\n",
      "Epoch 4/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 3.0514 - val_loss: 3.0333\n",
      "\n",
      "Epoch 00004: loss improved from 3.05185 to 3.05142, saving model to Weights-LSTM-improvement-004-3.05142-bigger.hdf5\n",
      "Epoch 5/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 3.0311 - val_loss: 2.9791\n",
      "\n",
      "Epoch 00005: loss improved from 3.05142 to 3.03107, saving model to Weights-LSTM-improvement-005-3.03107-bigger.hdf5\n",
      "Epoch 6/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 2.8554 - val_loss: 2.6966\n",
      "\n",
      "Epoch 00006: loss improved from 3.03107 to 2.85542, saving model to Weights-LSTM-improvement-006-2.85542-bigger.hdf5\n",
      "Epoch 7/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 2.5104 - val_loss: 2.4479\n",
      "\n",
      "Epoch 00007: loss improved from 2.85542 to 2.51045, saving model to Weights-LSTM-improvement-007-2.51045-bigger.hdf5\n",
      "Epoch 8/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 2.1647 - val_loss: 2.1067\n",
      "\n",
      "Epoch 00008: loss improved from 2.51045 to 2.16470, saving model to Weights-LSTM-improvement-008-2.16470-bigger.hdf5\n",
      "Epoch 9/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 1.6642 - val_loss: 1.6231\n",
      "\n",
      "Epoch 00009: loss improved from 2.16470 to 1.66423, saving model to Weights-LSTM-improvement-009-1.66423-bigger.hdf5\n",
      "Epoch 10/30\n",
      "47518/47518 [==============================] - 110s 2ms/step - loss: 1.1145 - val_loss: 1.2259\n",
      "\n",
      "Epoch 00010: loss improved from 1.66423 to 1.11448, saving model to Weights-LSTM-improvement-010-1.11448-bigger.hdf5\n",
      "Epoch 11/30\n",
      "47518/47518 [==============================] - 110s 2ms/step - loss: 0.6579 - val_loss: 0.9112\n",
      "\n",
      "Epoch 00011: loss improved from 1.11448 to 0.65789, saving model to Weights-LSTM-improvement-011-0.65789-bigger.hdf5\n",
      "Epoch 12/30\n",
      "47518/47518 [==============================] - 110s 2ms/step - loss: 0.3635 - val_loss: 0.7922\n",
      "\n",
      "Epoch 00012: loss improved from 0.65789 to 0.36349, saving model to Weights-LSTM-improvement-012-0.36349-bigger.hdf5\n",
      "Epoch 13/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.2148 - val_loss: 0.7359\n",
      "\n",
      "Epoch 00013: loss improved from 0.36349 to 0.21478, saving model to Weights-LSTM-improvement-013-0.21478-bigger.hdf5\n",
      "Epoch 14/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.1461 - val_loss: 0.7223\n",
      "\n",
      "Epoch 00014: loss improved from 0.21478 to 0.14607, saving model to Weights-LSTM-improvement-014-0.14607-bigger.hdf5\n",
      "Epoch 15/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.1205 - val_loss: 0.6993\n",
      "\n",
      "Epoch 00015: loss improved from 0.14607 to 0.12051, saving model to Weights-LSTM-improvement-015-0.12051-bigger.hdf5\n",
      "Epoch 16/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.1002 - val_loss: 0.7247\n",
      "\n",
      "Epoch 00016: loss improved from 0.12051 to 0.10017, saving model to Weights-LSTM-improvement-016-0.10017-bigger.hdf5\n",
      "Epoch 17/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0879 - val_loss: 0.7216\n",
      "\n",
      "Epoch 00017: loss improved from 0.10017 to 0.08792, saving model to Weights-LSTM-improvement-017-0.08792-bigger.hdf5\n",
      "Epoch 18/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0831 - val_loss: 0.7299\n",
      "\n",
      "Epoch 00018: loss improved from 0.08792 to 0.08306, saving model to Weights-LSTM-improvement-018-0.08306-bigger.hdf5\n",
      "Epoch 19/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0752 - val_loss: 0.7124\n",
      "\n",
      "Epoch 00019: loss improved from 0.08306 to 0.07524, saving model to Weights-LSTM-improvement-019-0.07524-bigger.hdf5\n",
      "Epoch 20/30\n",
      "47518/47518 [==============================] - 169s 4ms/step - loss: 0.0753 - val_loss: 0.6869\n",
      "\n",
      "Epoch 00020: loss did not improve from 0.07524\n",
      "Epoch 21/30\n",
      "47518/47518 [==============================] - 161s 3ms/step - loss: 0.0758 - val_loss: 0.6806\n",
      "\n",
      "Epoch 00021: loss did not improve from 0.07524\n",
      "Epoch 22/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0651 - val_loss: 0.7169\n",
      "\n",
      "Epoch 00022: loss improved from 0.07524 to 0.06513, saving model to Weights-LSTM-improvement-022-0.06513-bigger.hdf5\n",
      "Epoch 23/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0708 - val_loss: 0.7092\n",
      "\n",
      "Epoch 00023: loss did not improve from 0.06513\n",
      "Epoch 24/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0639 - val_loss: 0.7089\n",
      "\n",
      "Epoch 00024: loss improved from 0.06513 to 0.06392, saving model to Weights-LSTM-improvement-024-0.06392-bigger.hdf5\n",
      "Epoch 25/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0626 - val_loss: 0.7016\n",
      "\n",
      "Epoch 00025: loss improved from 0.06392 to 0.06261, saving model to Weights-LSTM-improvement-025-0.06261-bigger.hdf5\n",
      "Epoch 26/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0585 - val_loss: 0.7286\n",
      "\n",
      "Epoch 00026: loss improved from 0.06261 to 0.05851, saving model to Weights-LSTM-improvement-026-0.05851-bigger.hdf5\n",
      "Epoch 27/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0539 - val_loss: 0.7690\n",
      "\n",
      "Epoch 00027: loss improved from 0.05851 to 0.05389, saving model to Weights-LSTM-improvement-027-0.05389-bigger.hdf5\n",
      "Epoch 28/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0526 - val_loss: 0.7042\n",
      "\n",
      "Epoch 00028: loss improved from 0.05389 to 0.05264, saving model to Weights-LSTM-improvement-028-0.05264-bigger.hdf5\n",
      "Epoch 29/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0479 - val_loss: 0.6917\n",
      "\n",
      "Epoch 00029: loss improved from 0.05264 to 0.04790, saving model to Weights-LSTM-improvement-029-0.04790-bigger.hdf5\n",
      "Epoch 30/30\n",
      "47518/47518 [==============================] - 109s 2ms/step - loss: 0.0460 - val_loss: 0.7389\n",
      "\n",
      "Epoch 00030: loss improved from 0.04790 to 0.04599, saving model to Weights-LSTM-improvement-030-0.04599-bigger.hdf5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7f4079c11190>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model :\n",
    "model_params = {'epochs':30,\n",
    "                'batch_size':128,\n",
    "                'callbacks':callbacks_list,\n",
    "                'verbose':1,\n",
    "                'validation_split':0.2,\n",
    "                'validation_data':None,\n",
    "                'shuffle': True,\n",
    "                'initial_epoch':0,\n",
    "                'steps_per_epoch':None,\n",
    "                'validation_steps':None}\n",
    "model.fit(X,\n",
    "          y,\n",
    "          epochs = model_params['epochs'],\n",
    "           batch_size = model_params['batch_size'],\n",
    "           callbacks= model_params['callbacks'],\n",
    "           verbose = model_params['verbose'],\n",
    "           validation_split = model_params['validation_split'],\n",
    "           validation_data = model_params['validation_data'],\n",
    "           shuffle = model_params['shuffle'],\n",
    "           initial_epoch = model_params['initial_epoch'],\n",
    "           steps_per_epoch = model_params['steps_per_epoch'],\n",
    "           validation_steps = model_params['validation_steps'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load wights file :\n",
    "wights_file = 'Weights-LSTM-improvement-030-0.04599-bigger.hdf5' # weights file path\n",
    "model.load_weights(wights_file)\n",
    "model.compile(loss = 'categorical_crossentropy', optimizer = 'adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed : \n",
      "\"  vai. vai senta, vai senta, oi senta pra matar saudade. vai senta, vai senta, oi do jeitinho que tu  \"\n",
      "\n",
      "sabe. vai senta, vai senta, oi senta pra matar saudade. jittapê, kekel e kevinho, kevinho, jottapê e kekel. não preciso nem falar né?. tá bom eu vou falar, isso é hit maker fiu. vem sentando, vem sentando, vem sentando, vem. se acabou amor. que seja eterna sacanagem. explodiu bebê\"\n",
      "\"3\" \"alô. por que você não me atendeu\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "# set a random seed :\n",
    "start = np.random.randint(0, len(data_X)-1)\n",
    "pattern = data_X[start]\n",
    "print('Seed : ')\n",
    "print(\"\\\"\",''.join([int_chars[value] for value in pattern]), \"\\\"\\n\")\n",
    "# How many characters you want to generate\n",
    "generated_characters = 320\n",
    "# Generate Charachters :\n",
    "for i in range(generated_characters):\n",
    "    x = np.reshape(pattern, ( 1, len(pattern), 1))\n",
    "    x = x / float(n_vocab)\n",
    "    prediction = model.predict(x,verbose = 0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_chars[index]\n",
    "    #seq_in = [int_chars[value] for value in pattern]\n",
    "    sys.stdout.write(result)\n",
    "    pattern.append(index)\n",
    "    pattern = pattern[1:len(pattern)]\n",
    "print('\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
